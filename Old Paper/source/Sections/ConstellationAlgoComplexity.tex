Constellation's increased speed is due in part to its superior algorithmic efficiency. First consider its core object detection system, a neural network. The network is pre-trained, so in the analysis of its efficiency, the training steps, including the backpropagation algorithm used to fine tune its weights, will be ignored, as at run-time, they will not play any role in the time and number of calculations it takes to determine the nature of an object. Thus, the complexity of identifying an object is only the complexity of feeding the data from the image through the neural network and producing an output. The derivations and declarations of variables used to figure the complexity of the feed-forward algorithm are:
$$\text{\# layers}=\text{constant }c \enspace (\text{in our implementation }c=5)$$
$$\frac{\text{\# neurons}}{\text{layer}}=\sqrt{n}\text{ for $n$ elements in an input feature vector}$$
$$\frac{\text{\# calculations}}{\text{neuron}}=n+5$$
$$\text{\# times object detection called}=(w-\sqrt{n})(h-\sqrt{n})$$
$$\text{(for image width }w\text{, and image height }h;$$
$$\text{in our implementation }w=1080\text{, }h=720)$$
Thus, it can be determined that the number of calculations required to determine the classification of a set of $n$ inputs is $5\sqrt{n}n+25\sqrt{n}$, and further, the complexity of classifying all objects in an image is $O(n^2)$. 
% $O(5\sqrt{n}n+25\sqrt{n}

To perform similar tasks, a fairly naive implementation of a Structure-From-Motion pipeline would require $15n^3+c$ calculations, and a stereo vision system implementation would require $find\enspace this$ calculations, both far more than Constellation's mere $find\enspace this$ needed calculations for fairly standard object image size of $n=100$ pixels.