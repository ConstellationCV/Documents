\subsection{Neural Networks}
An Artificial Neural Network (ANN) is a computational model inspired by biological networks in the human brain which process large amounts of information.
\subsubsection{A Single Neuron}
The basic unit if a neural network is a neuron. A neuron is essentially a node in the graph which represents the neural network. Each neuron in an ANN, similar to a biological neuron, receives input from other neurons. Each input has an associated weight $W$, which is adjusted based on the importance of its corresponding input in the large picture of all the inputs to a neuron. Once it has acquired its inputs, a neuron will apply an activation function $f$, as shown below, to the weighted sum of its inputs to produce an output.
\input{Diagrams/Neuron.tex}

The function $f$ is a non-linear activation function. The role of the activation function is to create non-linearity in the output of a neuron, which is key because most real-world data in non linear and the purpose of neurons is to learn how to represent that data. Every activation function takes a single input and performs a mathematical operation on it in order to produce the final output. Common activation functions include:
\begin{itemize}
    \item Sigmoid: takes a value and squashes it to range 0-1. $\sigma(x) = \frac{1}{1+e^{-x}}$
    \item tahn: takes a value and squashes it to range [-1,1]. $\text{tahn}(x)=2\sigma(2x)-1$
    \item ReLU: stands for Rectified Linear Unit. Takes a value and replaces all negative values with 0. $f(x)=\text{max}(0,x)$
\end{itemize}

Constellation chooses to use sigmoid as the primary activation function due to its gradient nature, fixed range, and the fact that it is easier to differentiate values closer to one asympote or the other.
$$\text{input to activation}= (\sum\limits_{i=1}^n (x_i\times w_i))+b$$
$$\text{activation }f(x) = \frac{1}{1+e^{-x}}$$
$$\text{output}=\frac{1}{1+e^{-((\sum\limits_{i=1}^n (x_i\times w_i))+b))}}$$
\subsubsection{Feedforward Networks}
The feedforward neural network is the simplest and most widely applied type of ANN devised. Feedforward nets contain several neurons sorted into layers. Nodes from adjacent layers have connections, represented as edges on the graph, between them. Every connection, as discussed earlier, has a weight associated with it. \\
\input{Diagrams/FeedForwardNet.tex}

A feedforward network can consist of three types of nodes:
\begin{enumerate}
    \item Input Nodes - Provide information from the outside world to the network, the set of which is called the "Input Layer". None of these nodes perform any computation.
    \item Hidden Nodes - Have no direct connection with outside world and only perform computations and transfer information from the input nodes to the output nodes. A set of these nodes in called a "Hidden Layer". Feedforward nets can have any number of hidden layers.
    \item Output Nodes - Collectively called the "Output Layer", these nodes perform computations and also transfer information from the network to the outside world
\end{enumerate}

In a feedforward network, information only moves forward, through the input nodes, hidden nodes, and then finally on to output nodes. There is no cycling or looping back through the network. Below is an abridged psuedocode version of the feedforward algorithm implemented in Constellation:
\input{Sections/FFAlgo.tex}

\subsubsection{Training Using Back-Propagation}
It is possible to train a Multi-Layer Perceptron (MLP) Neural Network, a network which has at least one hidden layer, using a technique called back-propagation. Back-Propagation is a type of training approach which is called supervised learning, which means that the network is learning from labeled training data, where there is data is fed in, accompanied by classifications for each set of data which the network is told are correct. Each connection between nodes has a weight, which, as mentioned earlier, dictates the relevance of a specific input in the greater scope of all the inputs to a given neuron. The goal of learning is to assign correct weights to these edges in order to provide the most accurate classifications.

All weights are randomly assigned to begin with. Then, one by one, every training set is passed through the neural network, and, depending on how vast the difference desired and actual ouput of each layer, the error is propagated back to the previous layer. The error is noted and each weight is adjusted accordingly. This process is repeated using the provided data set until the network is outputting acceptably close classifications to the labeled classifications. Once this level is reached, the network can be passed previously unseen data and can use its trained weights to produce a classification. 

Below is an abridged psuedocode version of the backpropagation algorithm implemented in Constellation:
\input{Sections/BPAlgo.tex}