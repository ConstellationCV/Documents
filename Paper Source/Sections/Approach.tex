\subsection{Edge Detection}
\subsection{Edge Formulation}
\subsubsection{Determining Lines of Edges}
Before the discussion of how to determine which points a line consists of, it is important to discuss an efficient model for formulating a representation of a line which is represented by a set points. Consider constants $\alpha$ (alpha) and $\beta$ (beta) such that a following line exits:
$$y_i=\beta x_i+\alpha+\epsilon_i$$
where $y_i$ is the y value of point $i$, $x_i$ is the x value of $i$, and $\epsilon_i$ is the optimally small error term representing the imperfectness of the model to include that point. The problem now presents a textbook optimization problem; finding values of $\alpha$ and $\beta$ which produce the minimum $\epsilon$. Immediately, gradient descent comes to mind, however, a different approach to this optimization problem was taken, for reasons explained later. In order to find the alpha and beta values, the error term must be first defined. The error should increase as points are further away from the line which is created. This logic would lend itself well to a representation as simply the difference between the $y_i$ value of the point $i$ and the $y_l$ value of the line $l$ at $x_i$. If this approach is taken, however, when the error term $\epsilon$ is calculated, if one $x_1$ is very positive and another $x_2$ is very negative, meaning that $(x_1,y_1)$ lies far above the line and $(x_2,y_2)$ lies far below the line, then their errors might cancel out. So, instead, the squared error $\epsilon$ of each point  is considered, providing a total error $E$, represented as: $$\epsilon(y_i,y_l)=(y_i-y_l)^2$$
$$E(m,b) = \sum\limits_{i=1}^n \epsilon(y_i,mx_i+b)$$

Explicitly writing out this function composition we get the following, which can be expanded as shown:
$$E(m,b)=\sum\limits_{i=1}^n(y_i-mx_i-b)^2$$
$$E(m,b)=\sum\limits_{i=1}^n(y^2_i+m^2x^2_i+b^2-2mx_iy_i-2by_i+2bmx_i)$$
$$E(m,b)=\sum\limits_{i=1}^n y^2_i+ \sum\limits_{i=1}^n m^2x^2_i+ \sum\limits_{i=1}^n b^2-\sum\limits_{i=1}^n 2mx_iy_i-\sum\limits_{i=1}^n 2by_i+ \sum\limits_{i=1}^n 2bmx_i$$

For ease of readability the six terms are somewhat replaced [by $X=\sum x_i, Y=\sum y_i, A=\sum x^2_i, B=\sum x_iy_i, C=\sum y_i^2, N=\text{number of points}$] to produce:
$$E(m,b) = C + m^2A+b^2N - 2mB - 2bY + 2bmX$$

In order to find the values of $m$ and $b$ which minimize the error $E$, the relationships between those constants and $E$ must be found. This is can simply be done by taking the derivates of $E$ with respect to $m$ and $b$, as shown below. And since it is known that if there is an absolute minimum value of $E$, then it will occur at $\frac{\partial E}{\partial m}=0$ and $\frac{\partial E}{\partial b}=0$, values which make those relationships as close to 0 as possible are to be found.

$$\frac{\partial E}{\partial m}=2mA-2B+2bX=0 \enspace\text{(1)}$$
$$\frac{\partial E}{\partial b}=2bN-2Y+2mX=0 \enspace\text{(2)}$$

Now these equations can be solved by isolating for $b$ in equation $(2)$, and substituting that expression for all $b$ into equation $(1)$ to solve for $m$, producing (note - $\bar x$ means the average of all the $x$'s):
$$b=\frac{Y-mX}{N}=\frac{\sum y_i-m\sum x_i}{N}$$
$$m=\frac{NB-XY}{NA-X^2}=\frac{N\sum x_iy_i-\sum x_i\sum y_i}{N\sum x^2_i-(\sum x_i)^2}=\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum(x-\bar x)^2}$$

The reason this approach was taken as opposed to a gradient descent one is due to maximum likelihood estimation. A sample distribution of data $v_1,\dots,v_n$ which depends on an unknown parameter $\theta$ can be expressed as $p(v_1,\dots,v_n \mid \theta)$. If theta is unkown, this could conversely be represented as the likelihood of $\theta$ being true given the data; $L(\theta \mid v_1,\dots v_n)$. Given this, it can be seen that the most likely to be true theta is the $\theta$ which maximizes the likelihood function. As seen ealier, a linear regression model with the least $E$ should have a mean $\epsilon$ of 0, because it best distributes points above and below the line of best fit equally, with most points within a standard deviation of $\sigma$ away from the line. If this is the case, then it can be seen that the likelihood of $\alpha$ and $\beta$ being the best based on seeing a point $(x_i,y_i)$ is:
$$L(\alpha,\beta \mid x_i,y_i, \sigma)=\frac{1}{\sqrt{2\pi \sigma}}e^{-(y_i-\alpha-\beta x_i)^2/2\sigma ^2}$$

While gradient descent is more efficient than this approach, gradient descent provides $\alpha$ and $\beta$ values which do not maximize the likelihood function, for gradient descent only moves opposite the gradient till a given convergence threshold which must be greater than 0, thus never actually finding the absolute minimum, unlike this approach which does computationally find the absolute minimum $\epsilon$ by finding exact values of $\alpha$ and $\beta$.

As discussed later, Constellation's edge detection algorithm operates based on the angle of change between the line of best fit of a set $S=\{(x_1,y_1),\dots,(x_i,y_i)\}$ and set $S'=\{(x_1,y_1),\dots,(x_{i+1},y_{i+1})\}$. The angle of change, however, can only be measured between two lines which share an endpoint, so the current model must be adapted to represent only a line which begins at a given point. Logically, it makes sense to choose the first point $(x_1,y_1)$ considered when building the line to be the common anchor point, as it will always be the first point in any line built on $S$ or any $S'$. From there, the dataset is transformed to form $S^T=\{(u_i,v_i)=(x_i,y_i)-(x_1,y_1)\} \enspace \forall \text{ points}\enspace i$. So now the model to be fit is no longer $y_i=\beta x_i + \alpha + \epsilon_i$, but instead $y_i-y_1=\beta(x_i-x_0)+\epsilon_i$, effectively trying to fit a line with no intercept.
\subsubsection{Determining Which Points Lie in Line}
\subsection{Distance Estimation}
\input{Sections/DistanceEstimationApproach.tex}